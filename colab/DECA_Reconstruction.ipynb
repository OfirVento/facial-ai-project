{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß¨ Facial AI Platform ‚Äî DECA Face Reconstruction\n",
        "\n",
        "This notebook reconstructs a 3D FLAME mesh + texture from your photos.\n",
        "\n",
        "**What you need:**\n",
        "1. 1-3 face photos (front required, left 45¬∞ and right 45¬∞ optional)\n",
        "2. FLAME 2020 model file (download from https://flame.is.tue.mpg.de/)\n",
        "\n",
        "**What you get:**\n",
        "- `face_mesh.obj` ‚Äî FLAME topology 3D mesh\n",
        "- `face_texture.png` ‚Äî 1024x1024 albedo texture map\n",
        "- `face_normal.png` ‚Äî Normal map for surface detail\n",
        "- `face_params.json` ‚Äî FLAME shape/expression parameters\n",
        "\n",
        "Upload these files to your Facial AI Platform web app."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n",
        "import torch\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\"}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install DECA dependencies\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q face-alignment opencv-python-headless scikit-image\n",
        "!pip install -q pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt241/download.html\n",
        "!pip install -q chumpy\n",
        "\n",
        "# Clone DECA\n",
        "!git clone https://github.com/yfeng95/DECA.git\n",
        "%cd DECA\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "print('‚úÖ Dependencies installed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download DECA pretrained model\n",
        "!mkdir -p data\n",
        "!gdown --id 1rp8kdyLPvErw2dTmqtjISRVvQLj6Yzje -O data/deca_model.tar\n",
        "\n",
        "# You need to upload the FLAME model manually\n",
        "# Download from: https://flame.is.tue.mpg.de/\n",
        "# Upload generic_model.pkl to DECA/data/\n",
        "\n",
        "print('‚úÖ DECA model downloaded')\n",
        "print('‚ö†Ô∏è  Now upload FLAME model (generic_model.pkl) to DECA/data/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload FLAME Model\n",
        "\n",
        "Upload the `generic_model.pkl` file you downloaded from flame.is.tue.mpg.de"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Check if FLAME model already exists\n",
        "flame_path = 'data/generic_model.pkl'\n",
        "if not os.path.exists(flame_path):\n",
        "    print('Please upload generic_model.pkl from FLAME website:')\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        os.rename(filename, flame_path)\n",
        "        print(f'‚úÖ FLAME model saved to {flame_path}')\n",
        "else:\n",
        "    print(f'‚úÖ FLAME model already exists at {flame_path}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Upload Your Face Photos\n",
        "\n",
        "Upload 1-3 photos:\n",
        "- **Required:** Front-facing, neutral expression\n",
        "- **Optional:** Left 45¬∞, Right 45¬∞\n",
        "\n",
        "Tips:\n",
        "- Diffuse, even lighting (no harsh shadows)\n",
        "- Neutral expression, mouth closed\n",
        "- Hair pulled back from face\n",
        "- High resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Create input directory\n",
        "INPUT_DIR = '/content/face_input'\n",
        "os.makedirs(INPUT_DIR, exist_ok=True)\n",
        "\n",
        "print('üì∏ Upload your face photos (1-3 images):')\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename, data in uploaded.items():\n",
        "    dest = os.path.join(INPUT_DIR, filename)\n",
        "    with open(dest, 'wb') as f:\n",
        "        f.write(data)\n",
        "    print(f'  ‚úÖ Saved: {filename} ({len(data)/1024:.0f} KB)')\n",
        "\n",
        "print(f'\\nüìÅ {len(uploaded)} photo(s) uploaded to {INPUT_DIR}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run DECA Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/DECA')\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "# Run DECA reconstruction\n",
        "OUTPUT_DIR = '/content/face_output'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "from decalib.deca import DECA\n",
        "from decalib.utils.config import cfg as deca_cfg\n",
        "from decalib.datasets import datasets\n",
        "\n",
        "# Initialize DECA\n",
        "deca_cfg.model.use_tex = True\n",
        "deca_cfg.rasterizer_type = 'pytorch3d'\n",
        "deca = DECA(config=deca_cfg, device='cuda')\n",
        "\n",
        "print('‚úÖ DECA model loaded')\n",
        "\n",
        "# Process each photo\n",
        "input_files = sorted([f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "print(f'Processing {len(input_files)} image(s)...')\n",
        "\n",
        "all_params = []\n",
        "\n",
        "for i, filename in enumerate(input_files):\n",
        "    img_path = os.path.join(INPUT_DIR, filename)\n",
        "    print(f'\\n--- Processing {filename} ({i+1}/{len(input_files)}) ---')\n",
        "\n",
        "    # Load and preprocess\n",
        "    testdata = datasets.TestData(img_path, iscrop=True, face_detector='fan', sample_step=1)\n",
        "    if len(testdata) == 0:\n",
        "        print(f'  ‚ö†Ô∏è No face detected in {filename}, skipping')\n",
        "        continue\n",
        "\n",
        "    images = testdata[0]['image'].unsqueeze(0).to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        codedict = deca.encode(images)\n",
        "        opdict, visdict = deca.decode(codedict)\n",
        "\n",
        "    # Extract parameters\n",
        "    params = {\n",
        "        'shape': codedict['shape'].cpu().numpy().tolist()[0],\n",
        "        'exp': codedict['exp'].cpu().numpy().tolist()[0],\n",
        "        'pose': codedict['pose'].cpu().numpy().tolist()[0],\n",
        "        'cam': codedict['cam'].cpu().numpy().tolist()[0],\n",
        "        'light': codedict['light'].cpu().numpy().tolist()[0] if 'light' in codedict else None,\n",
        "        'tex': codedict['tex'].cpu().numpy().tolist()[0] if 'tex' in codedict else None,\n",
        "        'source_image': filename\n",
        "    }\n",
        "    all_params.append(params)\n",
        "\n",
        "    # Get mesh vertices and faces\n",
        "    vertices = opdict['verts'].cpu().numpy()[0]\n",
        "    faces = deca.flame.faces_tensor.cpu().numpy()\n",
        "\n",
        "    print(f'  Mesh: {vertices.shape[0]} vertices, {faces.shape[0]} faces')\n",
        "    print(f'  Shape params: {len(params[\"shape\"])}')\n",
        "    print(f'  Expression params: {len(params[\"exp\"])}')\n",
        "\n",
        "print(f'\\n‚úÖ Reconstruction complete for {len(all_params)} image(s)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Export Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use the first (front) image as primary reconstruction\n",
        "primary_idx = 0\n",
        "\n",
        "# Re-run decode for primary image to get mesh data\n",
        "testdata = datasets.TestData(\n",
        "    os.path.join(INPUT_DIR, input_files[primary_idx]),\n",
        "    iscrop=True, face_detector='fan', sample_step=1\n",
        ")\n",
        "images = testdata[0]['image'].unsqueeze(0).to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    codedict = deca.encode(images)\n",
        "    opdict, visdict = deca.decode(codedict)\n",
        "\n",
        "vertices = opdict['verts'].cpu().numpy()[0]\n",
        "faces = deca.flame.faces_tensor.cpu().numpy()\n",
        "\n",
        "# Get UV coordinates from FLAME\n",
        "try:\n",
        "    uvs = deca.flame.vt.cpu().numpy() if hasattr(deca.flame, 'vt') else None\n",
        "    uv_faces = deca.flame.ft.cpu().numpy() if hasattr(deca.flame, 'ft') else None\n",
        "except:\n",
        "    uvs = None\n",
        "    uv_faces = None\n",
        "\n",
        "# === Export OBJ mesh ===\n",
        "obj_path = os.path.join(OUTPUT_DIR, 'face_mesh.obj')\n",
        "mtl_path = os.path.join(OUTPUT_DIR, 'face_mesh.mtl')\n",
        "\n",
        "with open(obj_path, 'w') as f:\n",
        "    f.write('# DECA FLAME Reconstruction\\n')\n",
        "    f.write(f'# Vertices: {vertices.shape[0]}\\n')\n",
        "    f.write(f'# Faces: {faces.shape[0]}\\n')\n",
        "    f.write(f'mtllib face_mesh.mtl\\n')\n",
        "    f.write(f'usemtl face_material\\n\\n')\n",
        "\n",
        "    # Vertices\n",
        "    for v in vertices:\n",
        "        f.write(f'v {v[0]:.6f} {v[1]:.6f} {v[2]:.6f}\\n')\n",
        "\n",
        "    # UVs\n",
        "    if uvs is not None:\n",
        "        for uv in uvs:\n",
        "            f.write(f'vt {uv[0]:.6f} {uv[1]:.6f}\\n')\n",
        "\n",
        "    # Faces (1-indexed)\n",
        "    if uvs is not None and uv_faces is not None:\n",
        "        for fi, face in enumerate(faces):\n",
        "            uv_face = uv_faces[fi] if fi < len(uv_faces) else face\n",
        "            f.write(f'f {face[0]+1}/{uv_face[0]+1} {face[1]+1}/{uv_face[1]+1} {face[2]+1}/{uv_face[2]+1}\\n')\n",
        "    else:\n",
        "        for face in faces:\n",
        "            f.write(f'f {face[0]+1} {face[1]+1} {face[2]+1}\\n')\n",
        "\n",
        "# MTL file\n",
        "with open(mtl_path, 'w') as f:\n",
        "    f.write('newmtl face_material\\n')\n",
        "    f.write('Ka 0.2 0.2 0.2\\n')\n",
        "    f.write('Kd 0.8 0.8 0.8\\n')\n",
        "    f.write('map_Kd face_texture.png\\n')\n",
        "    f.write('bump face_normal.png\\n')\n",
        "\n",
        "print(f'‚úÖ Mesh exported: {obj_path}')\n",
        "print(f'   {vertices.shape[0]} vertices, {faces.shape[0]} faces')\n",
        "\n",
        "# === Export texture ===\n",
        "tex_path = os.path.join(OUTPUT_DIR, 'face_texture.png')\n",
        "\n",
        "if 'uv_texture_gt' in visdict:\n",
        "    texture = visdict['uv_texture_gt'][0].cpu().numpy()\n",
        "    texture = (texture.transpose(1, 2, 0) * 255).astype(np.uint8)\n",
        "    Image.fromarray(texture).save(tex_path)\n",
        "    print(f'‚úÖ Texture exported: {tex_path} ({texture.shape[0]}x{texture.shape[1]})')\n",
        "elif opdict.get('albedo') is not None:\n",
        "    albedo = opdict['albedo'][0].cpu().numpy()\n",
        "    albedo = (albedo.transpose(1, 2, 0) * 255).astype(np.uint8)\n",
        "    Image.fromarray(albedo).save(tex_path)\n",
        "    print(f'‚úÖ Albedo texture exported: {tex_path}')\n",
        "else:\n",
        "    # Fallback: use input photo directly\n",
        "    src_img = cv2.imread(os.path.join(INPUT_DIR, input_files[primary_idx]))\n",
        "    src_img = cv2.cvtColor(src_img, cv2.COLOR_BGR2RGB)\n",
        "    src_img = cv2.resize(src_img, (1024, 1024))\n",
        "    Image.fromarray(src_img).save(tex_path)\n",
        "    print(f'‚úÖ Fallback texture from photo: {tex_path}')\n",
        "\n",
        "# === Export normal map from displacement ===\n",
        "normal_path = os.path.join(OUTPUT_DIR, 'face_normal.png')\n",
        "\n",
        "if 'displacement_map' in opdict:\n",
        "    disp = opdict['displacement_map'][0].cpu().numpy()\n",
        "    disp = (disp.transpose(1, 2, 0) * 127.5 + 127.5).astype(np.uint8)\n",
        "    Image.fromarray(disp).save(normal_path)\n",
        "    print(f'‚úÖ Normal/displacement map exported: {normal_path}')\n",
        "else:\n",
        "    # Generate normal map from vertices\n",
        "    normal_img = np.full((1024, 1024, 3), 128, dtype=np.uint8)\n",
        "    normal_img[:, :, 2] = 255  # Z-up default normal\n",
        "    Image.fromarray(normal_img).save(normal_path)\n",
        "    print(f'‚úÖ Default normal map exported: {normal_path}')\n",
        "\n",
        "# === Export parameters JSON ===\n",
        "params_path = os.path.join(OUTPUT_DIR, 'face_params.json')\n",
        "export_data = {\n",
        "    'vertex_count': int(vertices.shape[0]),\n",
        "    'face_count': int(faces.shape[0]),\n",
        "    'shape_params': all_params[primary_idx]['shape'],\n",
        "    'expression_params': all_params[primary_idx]['exp'],\n",
        "    'pose_params': all_params[primary_idx]['pose'],\n",
        "    'source_images': [p['source_image'] for p in all_params],\n",
        "    'reconstruction_method': 'DECA',\n",
        "    'flame_model': 'FLAME2020'\n",
        "}\n",
        "\n",
        "with open(params_path, 'w') as f:\n",
        "    json.dump(export_data, f, indent=2)\n",
        "\n",
        "print(f'‚úÖ Parameters exported: {params_path}')\n",
        "print(f'\\nüìÅ All files in {OUTPUT_DIR}:')\n",
        "for f in os.listdir(OUTPUT_DIR):\n",
        "    size = os.path.getsize(os.path.join(OUTPUT_DIR, f))\n",
        "    print(f'  {f} ({size/1024:.0f} KB)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Preview Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(16, 5))\n",
        "\n",
        "# Show source photo\n",
        "ax1 = fig.add_subplot(141)\n",
        "src = Image.open(os.path.join(INPUT_DIR, input_files[primary_idx]))\n",
        "ax1.imshow(src)\n",
        "ax1.set_title('Source Photo')\n",
        "ax1.axis('off')\n",
        "\n",
        "# Show 3D mesh\n",
        "ax2 = fig.add_subplot(142, projection='3d')\n",
        "ax2.plot_trisurf(vertices[:, 0], vertices[:, 1], vertices[:, 2],\n",
        "                 triangles=faces, color='#e8b89d', edgecolor='gray',\n",
        "                 linewidth=0.1, alpha=0.8)\n",
        "ax2.set_title('3D Mesh (Front)')\n",
        "ax2.view_init(elev=0, azim=0)\n",
        "ax2.axis('off')\n",
        "\n",
        "# Show texture\n",
        "ax3 = fig.add_subplot(143)\n",
        "tex = Image.open(tex_path)\n",
        "ax3.imshow(tex)\n",
        "ax3.set_title('Texture Map')\n",
        "ax3.axis('off')\n",
        "\n",
        "# Show 3D mesh (profile)\n",
        "ax4 = fig.add_subplot(144, projection='3d')\n",
        "ax4.plot_trisurf(vertices[:, 0], vertices[:, 1], vertices[:, 2],\n",
        "                 triangles=faces, color='#e8b89d', edgecolor='gray',\n",
        "                 linewidth=0.1, alpha=0.8)\n",
        "ax4.set_title('3D Mesh (Profile)')\n",
        "ax4.view_init(elev=0, azim=90)\n",
        "ax4.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'preview.png'), dpi=150)\n",
        "plt.show()\n",
        "print('‚úÖ Preview saved')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Download Results\n",
        "\n",
        "Download the ZIP file and upload the contents to your Facial AI Platform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/facial_reconstruction.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "    for f in os.listdir(OUTPUT_DIR):\n",
        "        zf.write(os.path.join(OUTPUT_DIR, f), f)\n",
        "\n",
        "zip_size = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "print(f'üì¶ Created: facial_reconstruction.zip ({zip_size:.1f} MB)')\n",
        "print(f'\\nContents:')\n",
        "with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "    for info in zf.infolist():\n",
        "        print(f'  {info.filename} ({info.file_size/1024:.0f} KB)')\n",
        "\n",
        "print(f'\\n‚¨áÔ∏è  Downloading...')\n",
        "files.download(zip_path)\n",
        "\n",
        "print('\\n‚úÖ Done! Upload these files to your Facial AI Platform at:')\n",
        "print('   https://facial-ai-project.vercel.app')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
