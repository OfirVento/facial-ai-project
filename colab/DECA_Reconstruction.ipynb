{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üß¨ Facial AI Platform ‚Äî DECA + FLAME 2023 Reconstruction\n\nThis notebook reconstructs a 3D FLAME mesh + photorealistic texture from your photos using **FLAME 2023** (latest model with revised eye region and improved expressions).\n\n**What you need:**\n1. 1-3 face photos (front required, left 45¬∞ and right 45¬∞ optional)\n2. FLAME 2023 model files (download from https://flame.is.tue.mpg.de/):\n   - `generic_model.pkl` ‚Äî Core FLAME 2023 model (103 MB)\n   - `FLAME_masks.pkl` ‚Äî Vertex masks (1.1 MB)\n   - `FLAME_texture.npz` ‚Äî Texture space (1.2 GB, optional but recommended)\n   - `mediapipe_landmark_embedding.npz` ‚Äî MediaPipe mapping (3.1 KB)\n\n**What you get:**\n- `face_mesh.obj` ‚Äî FLAME 2023 topology 3D mesh (5,023 vertices)\n- `face_texture.png` ‚Äî 1024x1024 photorealistic albedo texture\n- `face_normal.png` ‚Äî Normal map for skin detail (pores, wrinkles)\n- `face_displacement.png` ‚Äî Displacement map for geometry detail\n- `face_params.json` ‚Äî FLAME shape/expression/pose parameters\n- `web/` folder ‚Äî Pre-converted files for the browser app (auto-generated)\n\nUpload these files to your Facial AI Platform web app at https://facial-ai-project.vercel.app"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Setup Environment & Check GPU"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check GPU availability\n!nvidia-smi\nimport torch\nprint(f'\\nPyTorch: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB')\nelse:\n    print('‚ö†Ô∏è  No GPU detected! Go to Runtime > Change runtime type > GPU')\n    print('   DECA reconstruction requires a GPU.')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Install DECA dependencies\n!pip install -q torch torchvision\n!pip install -q face-alignment opencv-python-headless scikit-image\n!pip install -q pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt241/download.html\n!pip install -q chumpy scipy\n\n# Clone DECA\nimport os\nif not os.path.exists('/content/DECA'):\n    !git clone https://github.com/yfeng95/DECA.git /content/DECA\n    %cd /content/DECA\n    !pip install -q -r requirements.txt\nelse:\n    %cd /content/DECA\n    print('DECA already cloned')\n\nprint('‚úÖ Dependencies installed')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Download DECA pretrained model\nimport os\nos.makedirs('data', exist_ok=True)\n\nif not os.path.exists('data/deca_model.tar'):\n    !gdown --id 1rp8kdyLPvErw2dTmqtjISRVvQLj6Yzje -O data/deca_model.tar\n    print('‚úÖ DECA pretrained model downloaded')\nelse:\n    print('‚úÖ DECA pretrained model already exists')\n\nprint('\\nüìã Next step: Upload your FLAME 2023 model files')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Upload FLAME 2023 Model Files\n\nUpload the files you downloaded from https://flame.is.tue.mpg.de/:\n\n| File | Required | Description |\n|------|----------|-------------|\n| `generic_model.pkl` | ‚úÖ Yes | Core FLAME 2023 model (103 MB) |\n| `FLAME_masks.pkl` | ‚úÖ Yes | Vertex region masks (1.1 MB) |\n| `FLAME_texture.npz` | üü° Recommended | Texture space for realistic skin (1.2 GB) |\n| `mediapipe_landmark_embedding.npz` | üü° Recommended | MediaPipe landmark mapping (3.1 KB) |"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from google.colab import files\nimport os\nimport shutil\n\n# Create FLAME data directory\nFLAME_DIR = '/content/DECA/data'\nos.makedirs(FLAME_DIR, exist_ok=True)\n\n# Define expected files and their destinations\nFLAME_FILES = {\n    'generic_model.pkl': {'dest': 'generic_model.pkl', 'required': True, 'desc': 'FLAME 2023 model'},\n    'FLAME_masks.pkl': {'dest': 'FLAME_masks.pkl', 'required': True, 'desc': 'Vertex masks'},\n    'FLAME_texture.npz': {'dest': 'FLAME_texture.npz', 'required': False, 'desc': 'Texture space'},\n    'mediapipe_landmark_embedding.npz': {'dest': 'mediapipe_landmark_embedding.npz', 'required': False, 'desc': 'MediaPipe mapping'},\n}\n\n# Check which files already exist\nmissing_required = []\nmissing_optional = []\nfor filename, info in FLAME_FILES.items():\n    dest_path = os.path.join(FLAME_DIR, info['dest'])\n    if os.path.exists(dest_path):\n        size = os.path.getsize(dest_path) / (1024 * 1024)\n        print(f'  ‚úÖ {info[\"desc\"]}: {filename} ({size:.1f} MB)')\n    elif info['required']:\n        missing_required.append(filename)\n    else:\n        missing_optional.append(filename)\n\nif missing_required or missing_optional:\n    if missing_required:\n        print(f'\\n‚ö†Ô∏è  Missing REQUIRED files: {\", \".join(missing_required)}')\n    if missing_optional:\n        print(f'‚ÑπÔ∏è  Missing optional files: {\", \".join(missing_optional)}')\n\n    print('\\nüì§ Please upload your FLAME files now:')\n    uploaded = files.upload()\n\n    for filename, data in uploaded.items():\n        # Match uploaded file to known FLAME files\n        matched = False\n        for known_name, info in FLAME_FILES.items():\n            if filename == known_name or filename.lower().replace('-', '_') == known_name.lower():\n                dest_path = os.path.join(FLAME_DIR, info['dest'])\n                with open(dest_path, 'wb') as f:\n                    f.write(data)\n                size = len(data) / (1024 * 1024)\n                print(f'  ‚úÖ Saved {info[\"desc\"]}: {dest_path} ({size:.1f} MB)')\n                matched = True\n                break\n        if not matched:\n            # Save unrecognized files too (might be renamed versions)\n            dest_path = os.path.join(FLAME_DIR, filename)\n            with open(dest_path, 'wb') as f:\n                f.write(data)\n            print(f'  üìÅ Saved: {dest_path} ({len(data)/1024/1024:.1f} MB)')\nelse:\n    print('\\n‚úÖ All FLAME files already present!')\n\n# Verify required files\nflame_model_path = os.path.join(FLAME_DIR, 'generic_model.pkl')\nif not os.path.exists(flame_model_path):\n    print('\\n‚ùå ERROR: generic_model.pkl is required! Please re-run this cell and upload it.')\nelse:\n    # Quick validation\n    import pickle\n    try:\n        with open(flame_model_path, 'rb') as f:\n            flame_data = pickle.load(f, encoding='latin1')\n        v_count = flame_data['v_template'].shape[0] if hasattr(flame_data['v_template'], 'shape') else len(flame_data['v_template'])\n        print(f'\\n‚úÖ FLAME 2023 model validated: {v_count} vertices')\n        if 'shapedirs' in flame_data:\n            shape_dims = flame_data['shapedirs'].shape[-1] if hasattr(flame_data['shapedirs'], 'shape') else '?'\n            print(f'   Shape parameters: {shape_dims}')\n        if 'exprdirs' in flame_data or 'expressionspace' in flame_data:\n            print(f'   Expression parameters: available')\n    except Exception as e:\n        print(f'‚ö†Ô∏è  Could not validate FLAME model: {e}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload Your Face Photos\n",
    "\n",
    "Upload 1-3 photos:\n",
    "- **Required:** Front-facing, neutral expression\n",
    "- **Optional:** Left 45¬∞, Right 45¬∞\n",
    "\n",
    "Tips:\n",
    "- Diffuse, even lighting (no harsh shadows)\n",
    "- Neutral expression, mouth closed\n",
    "- Hair pulled back from face\n",
    "- High resolution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create input directory\n",
    "INPUT_DIR = '/content/face_input'\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "\n",
    "print('üì∏ Upload your face photos (1-3 images):')\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename, data in uploaded.items():\n",
    "    dest = os.path.join(INPUT_DIR, filename)\n",
    "    with open(dest, 'wb') as f:\n",
    "        f.write(data)\n",
    "    print(f'  ‚úÖ Saved: {filename} ({len(data)/1024:.0f} KB)')\n",
    "\n",
    "print(f'\\nüìÅ {len(uploaded)} photo(s) uploaded to {INPUT_DIR}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Run DECA Reconstruction with FLAME 2023"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import sys\nsys.path.insert(0, '/content/DECA')\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport json\nimport pickle\n\n# Run DECA reconstruction\nOUTPUT_DIR = '/content/face_output'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nfrom decalib.deca import DECA\nfrom decalib.utils.config import cfg as deca_cfg\nfrom decalib.datasets import datasets\n\n# Initialize DECA with texture generation enabled\ndeca_cfg.model.use_tex = True\ndeca_cfg.rasterizer_type = 'pytorch3d'\ndeca = DECA(config=deca_cfg, device='cuda')\n\nprint(f'‚úÖ DECA model loaded')\nprint(f'   FLAME vertices: {deca.flame.v_template.shape[0]}')\n\n# Load vertex masks if available\nMASKS_PATH = '/content/DECA/data/FLAME_masks.pkl'\nvertex_masks = None\nif os.path.exists(MASKS_PATH):\n    with open(MASKS_PATH, 'rb') as f:\n        vertex_masks = pickle.load(f, encoding='latin1')\n    print(f'   Vertex masks: {list(vertex_masks.keys())}')\n\n# Process each photo\ninput_files = sorted([f for f in os.listdir(INPUT_DIR)\n                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.webp'))])\nprint(f'\\nProcessing {len(input_files)} image(s)...')\n\nall_params = []\nall_codedicts = []\n\nfor i, filename in enumerate(input_files):\n    img_path = os.path.join(INPUT_DIR, filename)\n    print(f'\\n--- Processing {filename} ({i+1}/{len(input_files)}) ---')\n\n    # Load and preprocess\n    testdata = datasets.TestData(img_path, iscrop=True, face_detector='fan', sample_step=1)\n    if len(testdata) == 0:\n        print(f'  ‚ö†Ô∏è No face detected in {filename}, skipping')\n        continue\n\n    images = testdata[0]['image'].unsqueeze(0).to('cuda')\n\n    with torch.no_grad():\n        codedict = deca.encode(images)\n        opdict, visdict = deca.decode(codedict)\n\n    all_codedicts.append(codedict)\n\n    # Extract parameters\n    params = {\n        'shape': codedict['shape'].cpu().numpy().tolist()[0],\n        'exp': codedict['exp'].cpu().numpy().tolist()[0],\n        'pose': codedict['pose'].cpu().numpy().tolist()[0],\n        'cam': codedict['cam'].cpu().numpy().tolist()[0],\n        'light': codedict['light'].cpu().numpy().tolist()[0] if 'light' in codedict else None,\n        'tex': codedict['tex'].cpu().numpy().tolist()[0] if 'tex' in codedict else None,\n        'detail': codedict['detail'].cpu().numpy().tolist()[0] if 'detail' in codedict else None,\n        'source_image': filename\n    }\n    all_params.append(params)\n\n    # Get mesh info\n    vertices = opdict['verts'].cpu().numpy()[0]\n    faces = deca.flame.faces_tensor.cpu().numpy()\n\n    print(f'  Mesh: {vertices.shape[0]} vertices, {faces.shape[0]} faces')\n    print(f'  Shape params: {len(params[\"shape\"])} dims')\n    print(f'  Expression params: {len(params[\"exp\"])} dims')\n    print(f'  Pose params: {len(params[\"pose\"])} dims (jaw + global rotation)')\n\n    # Show key shape values\n    shape_arr = np.array(params['shape'])\n    top_indices = np.argsort(np.abs(shape_arr))[-5:][::-1]\n    print(f'  Top shape components: {[(int(idx), f\"{shape_arr[idx]:.2f}\") for idx in top_indices]}')\n\nprint(f'\\n‚úÖ Reconstruction complete for {len(all_params)} image(s)')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Export Mesh, Textures & Parameters"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =====================================================================\n# EXPORT: Mesh + Textures + Parameters + Web-ready files\n# =====================================================================\n\nprimary_idx = 0  # Use first (front) image as primary\n\n# Re-run decode for primary image to get full mesh data\ntestdata = datasets.TestData(\n    os.path.join(INPUT_DIR, input_files[primary_idx]),\n    iscrop=True, face_detector='fan', sample_step=1\n)\nimages = testdata[0]['image'].unsqueeze(0).to('cuda')\n\nwith torch.no_grad():\n    codedict = deca.encode(images)\n    opdict, visdict = deca.decode(codedict)\n\nvertices = opdict['verts'].cpu().numpy()[0]\nfaces = deca.flame.faces_tensor.cpu().numpy()\n\n# Get UV coordinates from FLAME\ntry:\n    uvs = deca.flame.vt.cpu().numpy() if hasattr(deca.flame, 'vt') else None\n    uv_faces = deca.flame.ft.cpu().numpy() if hasattr(deca.flame, 'ft') else None\nexcept:\n    uvs = None\n    uv_faces = None\n\n# =====================================================================\n# 1. Export OBJ mesh\n# =====================================================================\nobj_path = os.path.join(OUTPUT_DIR, 'face_mesh.obj')\nmtl_path = os.path.join(OUTPUT_DIR, 'face_mesh.mtl')\n\nwith open(obj_path, 'w') as f:\n    f.write('# DECA + FLAME 2023 Reconstruction\\n')\n    f.write(f'# Vertices: {vertices.shape[0]}\\n')\n    f.write(f'# Faces: {faces.shape[0]}\\n')\n    f.write(f'# Generated by Facial AI Platform\\n')\n    f.write(f'mtllib face_mesh.mtl\\n')\n    f.write(f'usemtl face_material\\n\\n')\n\n    for v in vertices:\n        f.write(f'v {v[0]:.6f} {v[1]:.6f} {v[2]:.6f}\\n')\n\n    if uvs is not None:\n        for uv in uvs:\n            f.write(f'vt {uv[0]:.6f} {uv[1]:.6f}\\n')\n\n    # Compute and write vertex normals\n    from pytorch3d.structures import Meshes\n    mesh_p3d = Meshes(verts=[torch.tensor(vertices).float()],\n                      faces=[torch.tensor(faces).long()])\n    vnormals = mesh_p3d.verts_normals_packed().numpy()\n    for vn in vnormals:\n        f.write(f'vn {vn[0]:.6f} {vn[1]:.6f} {vn[2]:.6f}\\n')\n\n    for fi, face in enumerate(faces):\n        if uvs is not None and uv_faces is not None and fi < len(uv_faces):\n            uv_face = uv_faces[fi]\n            f.write(f'f {face[0]+1}/{uv_face[0]+1}/{face[0]+1} '\n                    f'{face[1]+1}/{uv_face[1]+1}/{face[1]+1} '\n                    f'{face[2]+1}/{uv_face[2]+1}/{face[2]+1}\\n')\n        else:\n            f.write(f'f {face[0]+1}//{face[0]+1} {face[1]+1}//{face[1]+1} {face[2]+1}//{face[2]+1}\\n')\n\nwith open(mtl_path, 'w') as f:\n    f.write('newmtl face_material\\n')\n    f.write('Ka 0.2 0.2 0.2\\n')\n    f.write('Kd 0.8 0.8 0.8\\n')\n    f.write('Ks 0.1 0.1 0.1\\n')\n    f.write('Ns 20.0\\n')\n    f.write('map_Kd face_texture.png\\n')\n    f.write('bump face_normal.png\\n')\n    f.write('disp face_displacement.png\\n')\n\nprint(f'‚úÖ Mesh: {obj_path} ({vertices.shape[0]} verts, {faces.shape[0]} faces)')\n\n# =====================================================================\n# 2. Export albedo texture (1024x1024)\n# =====================================================================\ntex_path = os.path.join(OUTPUT_DIR, 'face_texture.png')\nTEX_SIZE = 1024\n\nif 'uv_texture_gt' in visdict:\n    texture = visdict['uv_texture_gt'][0].cpu().numpy()\n    texture = (texture.transpose(1, 2, 0) * 255).astype(np.uint8)\n    texture = cv2.resize(texture, (TEX_SIZE, TEX_SIZE), interpolation=cv2.INTER_LANCZOS4)\n    Image.fromarray(texture).save(tex_path, quality=95)\n    print(f'‚úÖ Texture: {tex_path} ({TEX_SIZE}x{TEX_SIZE})')\nelif opdict.get('albedo') is not None:\n    albedo = opdict['albedo'][0].cpu().numpy()\n    albedo = (albedo.transpose(1, 2, 0) * 255).astype(np.uint8)\n    albedo = cv2.resize(albedo, (TEX_SIZE, TEX_SIZE), interpolation=cv2.INTER_LANCZOS4)\n    Image.fromarray(albedo).save(tex_path, quality=95)\n    print(f'‚úÖ Albedo texture: {tex_path} ({TEX_SIZE}x{TEX_SIZE})')\nelse:\n    # Generate texture from FLAME texture space if available\n    TEXTURE_SPACE_PATH = '/content/DECA/data/FLAME_texture.npz'\n    if os.path.exists(TEXTURE_SPACE_PATH) and 'tex' in codedict:\n        print('  Generating texture from FLAME texture space...')\n        tex_space = np.load(TEXTURE_SPACE_PATH)\n        if 'mean' in tex_space and 'tex_dir' in tex_space:\n            tex_params = codedict['tex'].cpu().numpy()[0]\n            mean_tex = tex_space['mean']\n            tex_dir = tex_space['tex_dir']\n            n_components = min(len(tex_params), tex_dir.shape[-1])\n            texture_flat = mean_tex + tex_dir[:, :, :n_components].dot(tex_params[:n_components])\n            texture = np.clip(texture_flat.reshape(TEX_SIZE, TEX_SIZE, 3), 0, 255).astype(np.uint8)\n            Image.fromarray(texture).save(tex_path, quality=95)\n            print(f'‚úÖ Texture from FLAME space: {tex_path}')\n        else:\n            print('  ‚ö†Ô∏è Unexpected texture space format, using photo fallback')\n    else:\n        # Fallback: project input photo\n        src_img = cv2.imread(os.path.join(INPUT_DIR, input_files[primary_idx]))\n        src_img = cv2.cvtColor(src_img, cv2.COLOR_BGR2RGB)\n        src_img = cv2.resize(src_img, (TEX_SIZE, TEX_SIZE), interpolation=cv2.INTER_LANCZOS4)\n        Image.fromarray(src_img).save(tex_path, quality=95)\n        print(f'‚úÖ Fallback texture from photo: {tex_path}')\n\n# =====================================================================\n# 3. Export normal map\n# =====================================================================\nnormal_path = os.path.join(OUTPUT_DIR, 'face_normal.png')\n\nif 'normal_images' in visdict:\n    normal_img = visdict['normal_images'][0].cpu().numpy()\n    normal_img = ((normal_img.transpose(1, 2, 0) + 1) * 0.5 * 255).astype(np.uint8)\n    normal_img = cv2.resize(normal_img, (TEX_SIZE, TEX_SIZE), interpolation=cv2.INTER_LANCZOS4)\n    Image.fromarray(normal_img).save(normal_path)\n    print(f'‚úÖ Normal map: {normal_path}')\nelse:\n    # Generate default tangent-space normal map\n    normal_img = np.full((TEX_SIZE, TEX_SIZE, 3), 128, dtype=np.uint8)\n    normal_img[:, :, 2] = 255  # Z-up default\n    Image.fromarray(normal_img).save(normal_path)\n    print(f'‚úÖ Default normal map: {normal_path}')\n\n# =====================================================================\n# 4. Export displacement map (if detail code available)\n# =====================================================================\ndisp_path = os.path.join(OUTPUT_DIR, 'face_displacement.png')\n\nif 'displacement_map' in opdict:\n    disp = opdict['displacement_map'][0, 0].cpu().numpy()\n    disp_normalized = ((disp - disp.min()) / (disp.max() - disp.min() + 1e-8) * 255).astype(np.uint8)\n    disp_normalized = cv2.resize(disp_normalized, (TEX_SIZE, TEX_SIZE), interpolation=cv2.INTER_LANCZOS4)\n    Image.fromarray(disp_normalized).save(disp_path)\n    print(f'‚úÖ Displacement map: {disp_path}')\nelif 'uv_detail_normals' in visdict:\n    detail = visdict['uv_detail_normals'][0].cpu().numpy()\n    detail = ((detail.transpose(1, 2, 0) + 1) * 0.5 * 255).astype(np.uint8)\n    detail = cv2.resize(detail, (TEX_SIZE, TEX_SIZE), interpolation=cv2.INTER_LANCZOS4)\n    Image.fromarray(detail).save(disp_path)\n    print(f'‚úÖ Detail normals as displacement: {disp_path}')\nelse:\n    # Flat displacement\n    Image.fromarray(np.full((TEX_SIZE, TEX_SIZE), 128, dtype=np.uint8)).save(disp_path)\n    print(f'‚úÖ Flat displacement map: {disp_path}')\n\n# =====================================================================\n# 5. Export FLAME parameters JSON\n# =====================================================================\nparams_path = os.path.join(OUTPUT_DIR, 'face_params.json')\n\nexport_data = {\n    'flame_version': 'FLAME 2023',\n    'reconstruction_method': 'DECA',\n    'vertex_count': int(vertices.shape[0]),\n    'face_count': int(faces.shape[0]),\n    'shape_params': all_params[primary_idx]['shape'],\n    'expression_params': all_params[primary_idx]['exp'],\n    'pose_params': all_params[primary_idx]['pose'],\n    'camera_params': all_params[primary_idx]['cam'],\n    'lighting_params': all_params[primary_idx]['light'],\n    'texture_params': all_params[primary_idx]['tex'],\n    'detail_params': all_params[primary_idx].get('detail'),\n    'source_images': [p['source_image'] for p in all_params],\n    'all_reconstructions': [{\n        'source': p['source_image'],\n        'shape': p['shape'],\n        'exp': p['exp'],\n        'pose': p['pose'],\n    } for p in all_params]\n}\n\nwith open(params_path, 'w') as f:\n    json.dump(export_data, f, indent=2)\n\nprint(f'‚úÖ Parameters: {params_path}')\n\n# =====================================================================\n# Summary\n# =====================================================================\nprint(f'\\n{\"=\"*60}')\nprint(f'üìÅ All exports in {OUTPUT_DIR}:')\nfor f_name in sorted(os.listdir(OUTPUT_DIR)):\n    size = os.path.getsize(os.path.join(OUTPUT_DIR, f_name))\n    unit = 'KB' if size < 1024*1024 else 'MB'\n    size_val = size/1024 if unit == 'KB' else size/(1024*1024)\n    print(f'  {f_name:30s} {size_val:8.1f} {unit}')\nprint(f'{\"=\"*60}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5.5: Generate Web-Ready Files\n\nConverts FLAME data to JSON + binary format that the browser app can load directly via `FlameMeshGenerator.loadFLAME()`."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =====================================================================\n# GENERATE WEB-READY FILES for the browser app\n# =====================================================================\n\nimport struct\nimport json\nimport numpy as np\nimport pickle\n\nWEB_DIR = os.path.join(OUTPUT_DIR, 'web')\nos.makedirs(WEB_DIR, exist_ok=True)\n\n# Load FLAME model data\nFLAME_PATH = '/content/DECA/data/generic_model.pkl'\nwith open(FLAME_PATH, 'rb') as f:\n    flame = pickle.load(f, encoding='latin1')\n\n# Convert chumpy arrays to numpy\ndef to_np(x):\n    return np.array(x) if hasattr(x, '__array__') else x\n\nv_template = to_np(flame['v_template']).astype(np.float32)\nshapedirs = to_np(flame['shapedirs']).astype(np.float32)\nfaces_arr = to_np(flame['f']).astype(np.int32)\n\n# Expression dirs (may be in different keys depending on FLAME version)\nexprdirs = None\nfor key in ['exprdirs', 'expressionspace', 'expression_dirs']:\n    if key in flame:\n        exprdirs = to_np(flame[key]).astype(np.float32)\n        break\n\nV = v_template.shape[0]  # 5023\nF = faces_arr.shape[0]    # ~9976\nSHAPE_COMPONENTS = min(50, shapedirs.shape[2])\nEXPR_COMPONENTS = min(50, exprdirs.shape[2]) if exprdirs is not None else 0\n\nprint(f'FLAME model loaded:')\nprint(f'  Vertices: {V}')\nprint(f'  Faces: {F}')\nprint(f'  Shape components: {shapedirs.shape[2]} (exporting {SHAPE_COMPONENTS})')\nprint(f'  Expression components: {EXPR_COMPONENTS}')\n\n# --- 1. Template vertices (Float32) ---\nv_template.tofile(os.path.join(WEB_DIR, 'flame_template_vertices.bin'))\n\n# --- 2. Shape basis (Float32): reshape to (V*3, N) then flatten ---\nshape_basis = shapedirs[:, :, :SHAPE_COMPONENTS].reshape(-1).astype(np.float32)\nshape_basis.tofile(os.path.join(WEB_DIR, 'flame_shape_basis.bin'))\n\n# --- 3. Expression basis (Float32) ---\nif exprdirs is not None:\n    expr_basis = exprdirs[:, :, :EXPR_COMPONENTS].reshape(-1).astype(np.float32)\n    expr_basis.tofile(os.path.join(WEB_DIR, 'flame_expression_basis.bin'))\n\n# --- 4. Face indices (Uint32) ---\nfaces_arr.astype(np.uint32).tofile(os.path.join(WEB_DIR, 'flame_faces.bin'))\n\n# --- 5. UV coordinates ---\nif uvs is not None:\n    uvs.astype(np.float32).tofile(os.path.join(WEB_DIR, 'flame_uv.bin'))\n\n# --- 6. Region mapping (52 clinical zones from vertex masks + position-based subdivision) ---\nregion_map = {}\n\n# Load vertex masks\nMASKS_PATH = '/content/DECA/data/FLAME_masks.pkl'\nif os.path.exists(MASKS_PATH):\n    with open(MASKS_PATH, 'rb') as f:\n        masks = pickle.load(f, encoding='latin1')\n\n    # FLAME masks typically contain: face, left_eyeball, right_eyeball, nose,\n    # right_eye_region, forehead, lips, right_ear, left_ear, left_eye_region,\n    # neck, scalp, boundary\n    flame_masks = {}\n    for key, val in masks.items():\n        flame_masks[key] = to_np(val).astype(int).tolist()\n        print(f'  Mask \"{key}\": {len(flame_masks[key])} vertices')\n\n    # Helper: filter vertices by position\n    def filter_by_pos(indices, x_min=-999, x_max=999, y_min=-999, y_max=999, z_min=-999, z_max=999):\n        result = []\n        for idx in indices:\n            x, y, z = v_template[idx]\n            if x_min <= x <= x_max and y_min <= y <= y_max and z_min <= z <= z_max:\n                result.append(int(idx))\n        return result\n\n    # Compute position statistics for subdivision thresholds\n    all_face_indices = flame_masks.get('face', list(range(V)))\n    face_verts = v_template[all_face_indices]\n    y_min_f, y_max_f = face_verts[:, 1].min(), face_verts[:, 1].max()\n    y_mid = (y_min_f + y_max_f) / 2\n    x_center = face_verts[:, 0].mean()\n\n    # full_face = all face mask vertices\n    region_map['full_face'] = flame_masks.get('face', list(range(V)))\n\n    # --- Forehead subdivisions ---\n    fh = flame_masks.get('forehead', [])\n    region_map['forehead'] = fh\n    region_map['forehead_left'] = filter_by_pos(fh, x_min=0.005)\n    region_map['forehead_right'] = filter_by_pos(fh, x_max=-0.005)\n    region_map['forehead_center'] = filter_by_pos(fh, x_min=-0.015, x_max=0.015)\n\n    # Brows: top portion of eye regions\n    left_eye = flame_masks.get('left_eye_region', [])\n    right_eye = flame_masks.get('right_eye_region', [])\n\n    if left_eye:\n        le_verts = v_template[left_eye]\n        le_y_mid = le_verts[:, 1].mean()\n        region_map['brow_left'] = filter_by_pos(left_eye, y_min=le_y_mid)\n        region_map['brow_inner_left'] = filter_by_pos(region_map['brow_left'], x_max=0.02)\n        region_map['eye_left_upper'] = filter_by_pos(left_eye, y_min=le_y_mid - 0.003, y_max=le_y_mid + 0.01)\n        region_map['eye_left_lower'] = filter_by_pos(left_eye, y_max=le_y_mid - 0.003)\n        region_map['eye_left_corner_inner'] = filter_by_pos(left_eye, x_max=0.01, y_min=le_y_mid-0.005, y_max=le_y_mid+0.005)\n        region_map['eye_left_corner_outer'] = filter_by_pos(left_eye, x_min=0.03, y_min=le_y_mid-0.005, y_max=le_y_mid+0.005)\n        region_map['under_eye_left'] = filter_by_pos(left_eye, y_max=le_y_mid - 0.008)\n        region_map['tear_trough_left'] = filter_by_pos(region_map.get('under_eye_left', []), x_max=0.02)\n\n    if right_eye:\n        re_verts = v_template[right_eye]\n        re_y_mid = re_verts[:, 1].mean()\n        region_map['brow_right'] = filter_by_pos(right_eye, y_min=re_y_mid)\n        region_map['brow_inner_right'] = filter_by_pos(region_map['brow_right'], x_min=-0.02)\n        region_map['eye_right_upper'] = filter_by_pos(right_eye, y_min=re_y_mid - 0.003, y_max=re_y_mid + 0.01)\n        region_map['eye_right_lower'] = filter_by_pos(right_eye, y_max=re_y_mid - 0.003)\n        region_map['eye_right_corner_inner'] = filter_by_pos(right_eye, x_min=-0.01, y_min=re_y_mid-0.005, y_max=re_y_mid+0.005)\n        region_map['eye_right_corner_outer'] = filter_by_pos(right_eye, x_max=-0.03, y_min=re_y_mid-0.005, y_max=re_y_mid+0.005)\n        region_map['under_eye_right'] = filter_by_pos(right_eye, y_max=re_y_mid - 0.008)\n        region_map['tear_trough_right'] = filter_by_pos(region_map.get('under_eye_right', []), x_min=-0.02)\n\n    # --- Nose subdivisions ---\n    nose = flame_masks.get('nose', [])\n    region_map['nose_bridge'] = filter_by_pos(nose, x_min=-0.01, x_max=0.01, y_min=0)\n    region_map['nose_bridge_upper'] = filter_by_pos(region_map['nose_bridge'], y_min=0.01)\n    region_map['nose_bridge_lower'] = filter_by_pos(region_map['nose_bridge'], y_max=0.01)\n    region_map['nose_tip'] = filter_by_pos(nose, y_max=0.005, z_min=0.03)\n    region_map['nose_tip_left'] = filter_by_pos(region_map['nose_tip'], x_min=0.002)\n    region_map['nose_tip_right'] = filter_by_pos(region_map['nose_tip'], x_max=-0.002)\n    region_map['nostril_left'] = filter_by_pos(nose, x_min=0.01, y_max=0)\n    region_map['nostril_right'] = filter_by_pos(nose, x_max=-0.01, y_max=0)\n    region_map['nose_dorsum'] = filter_by_pos(nose, x_min=-0.012, x_max=0.012)\n\n    # --- Lips subdivisions ---\n    lips = flame_masks.get('lips', [])\n    if lips:\n        lip_verts = v_template[lips]\n        lip_y_mid = lip_verts[:, 1].mean()\n        region_map['lip_upper'] = filter_by_pos(lips, y_min=lip_y_mid)\n        region_map['lip_upper_left'] = filter_by_pos(region_map['lip_upper'], x_min=0.005)\n        region_map['lip_upper_right'] = filter_by_pos(region_map['lip_upper'], x_max=-0.005)\n        region_map['lip_upper_center'] = filter_by_pos(region_map['lip_upper'], x_min=-0.008, x_max=0.008)\n        region_map['lip_lower'] = filter_by_pos(lips, y_max=lip_y_mid)\n        region_map['lip_lower_left'] = filter_by_pos(region_map['lip_lower'], x_min=0.005)\n        region_map['lip_lower_right'] = filter_by_pos(region_map['lip_lower'], x_max=-0.005)\n        region_map['lip_lower_center'] = filter_by_pos(region_map['lip_lower'], x_min=-0.008, x_max=0.008)\n        region_map['lip_corner_left'] = filter_by_pos(lips, x_min=0.02, y_min=lip_y_mid-0.003, y_max=lip_y_mid+0.003)\n        region_map['lip_corner_right'] = filter_by_pos(lips, x_max=-0.02, y_min=lip_y_mid-0.003, y_max=lip_y_mid+0.003)\n\n    # --- Cheeks, Jaw, Chin (from face mask, excluding other regions) ---\n    specific_indices = set()\n    for key in ['forehead', 'left_eye_region', 'right_eye_region', 'nose', 'lips',\n                'left_ear', 'right_ear', 'left_eyeball', 'right_eyeball', 'neck', 'scalp']:\n        if key in flame_masks:\n            specific_indices.update(flame_masks[key])\n\n    remaining_face = [i for i in all_face_indices if i not in specific_indices]\n\n    # Cheeks: lateral, mid-face height\n    region_map['cheek_left'] = filter_by_pos(remaining_face, x_min=0.02, y_min=-0.03, y_max=0.02)\n    region_map['cheek_right'] = filter_by_pos(remaining_face, x_max=-0.02, y_min=-0.03, y_max=0.02)\n    region_map['cheekbone_left'] = filter_by_pos(region_map['cheek_left'], y_min=0)\n    region_map['cheekbone_right'] = filter_by_pos(region_map['cheek_right'], y_min=0)\n    region_map['cheek_hollow_left'] = filter_by_pos(region_map['cheek_left'], y_max=0)\n    region_map['cheek_hollow_right'] = filter_by_pos(region_map['cheek_right'], y_max=0)\n\n    # Nasolabial\n    region_map['nasolabial_left'] = filter_by_pos(remaining_face, x_min=0.01, x_max=0.025, y_min=-0.04, y_max=0)\n    region_map['nasolabial_right'] = filter_by_pos(remaining_face, x_max=-0.01, x_min=-0.025, y_min=-0.04, y_max=0)\n\n    # Chin\n    chin_verts = filter_by_pos(remaining_face, y_max=-0.04, x_min=-0.03, x_max=0.03)\n    region_map['chin'] = chin_verts\n    region_map['chin_center'] = filter_by_pos(chin_verts, x_min=-0.01, x_max=0.01)\n    region_map['chin_left'] = filter_by_pos(chin_verts, x_min=0.01)\n    region_map['chin_right'] = filter_by_pos(chin_verts, x_max=-0.01)\n\n    # Jaw\n    jaw_verts = filter_by_pos(remaining_face, y_max=-0.02, x_min=0.03)\n    region_map['jaw_left'] = jaw_verts\n    region_map['jawline_left'] = filter_by_pos(jaw_verts, y_max=-0.04)\n    jaw_verts_r = filter_by_pos(remaining_face, y_max=-0.02, x_max=-0.03)\n    region_map['jaw_right'] = jaw_verts_r\n    region_map['jawline_right'] = filter_by_pos(jaw_verts_r, y_max=-0.04)\n\n    # Temples: above cheeks, lateral to forehead\n    region_map['temple_left'] = filter_by_pos(remaining_face, x_min=0.04, y_min=0.01)\n    region_map['temple_right'] = filter_by_pos(remaining_face, x_max=-0.04, y_min=0.01)\n\n    # Ears\n    region_map['ear_left'] = flame_masks.get('left_ear', [])\n    region_map['ear_right'] = flame_masks.get('right_ear', [])\n\n    # Neck\n    region_map['neck'] = flame_masks.get('neck', [])\n\nelse:\n    print('‚ö†Ô∏è  No vertex masks file found, region mapping will be position-based only')\n    # Fallback: purely position-based classification (less accurate)\n    for i in range(V):\n        x, y, z = v_template[i]\n        region_map.setdefault('full_face', []).append(i)\n\n# Save regions JSON\nwith open(os.path.join(WEB_DIR, 'flame_regions.json'), 'w') as f:\n    json.dump(region_map, f)\n\n# --- 7. MediaPipe mapping ---\nMP_PATH = '/content/DECA/data/mediapipe_landmark_embedding.npz'\nif os.path.exists(MP_PATH):\n    mp_data = np.load(MP_PATH, allow_pickle=True)\n    mp_mapping = {}\n    if 'lmk_face_idx' in mp_data:\n        mp_mapping['lmk_face_idx'] = mp_data['lmk_face_idx'].tolist()\n    if 'lmk_b_coords' in mp_data:\n        mp_mapping['lmk_b_coords'] = mp_data['lmk_b_coords'].tolist()\n    # Convert to closest vertex indices for simpler browser use\n    if 'lmk_face_idx' in mp_data and 'lmk_b_coords' in mp_data:\n        closest_vertices = []\n        for face_idx, bary in zip(mp_data['lmk_face_idx'], mp_data['lmk_b_coords']):\n            face = faces_arr[int(face_idx)]\n            # Pick the vertex with highest barycentric weight\n            max_bary_idx = np.argmax(bary)\n            closest_vertices.append(int(face[max_bary_idx]))\n        mp_mapping['closest_vertices'] = closest_vertices\n        mp_mapping['landmark_count'] = len(closest_vertices)\n\n    with open(os.path.join(WEB_DIR, 'flame_mediapipe_mapping.json'), 'w') as f:\n        json.dump(mp_mapping, f)\n    print(f'‚úÖ MediaPipe mapping: {len(mp_mapping.get(\"closest_vertices\", []))} landmarks')\n\n# --- 8. Template metadata JSON ---\ntemplate_meta = {\n    'vertexCount': int(V),\n    'faceCount': int(F),\n    'shapeComponents': int(SHAPE_COMPONENTS),\n    'expressionComponents': int(EXPR_COMPONENTS),\n    'flameVersion': 'FLAME 2023',\n    'hasUV': uvs is not None,\n    'hasMediaPipe': os.path.exists(MP_PATH),\n    'hasTexureSpace': os.path.exists('/content/DECA/data/FLAME_texture.npz'),\n    'regionCount': len(region_map),\n    'files': {\n        'vertices': 'flame_template_vertices.bin',\n        'shapeBasis': 'flame_shape_basis.bin',\n        'expressionBasis': 'flame_expression_basis.bin' if exprdirs is not None else None,\n        'faces': 'flame_faces.bin',\n        'uv': 'flame_uv.bin' if uvs is not None else None,\n        'regions': 'flame_regions.json',\n        'mediapipe': 'flame_mediapipe_mapping.json' if os.path.exists(MP_PATH) else None,\n    },\n    # Include reconstruction-specific params for this face\n    'reconstruction': {\n        'shapeParams': all_params[primary_idx]['shape'],\n        'expressionParams': all_params[primary_idx]['exp'],\n        'poseParams': all_params[primary_idx]['pose'],\n    }\n}\n\nwith open(os.path.join(WEB_DIR, 'flame_template.json'), 'w') as f:\n    json.dump(template_meta, f, indent=2)\n\n# Summary\nprint(f'\\n{\"=\"*60}')\nprint(f'üìÅ Web-ready files in {WEB_DIR}:')\ntotal_size = 0\nfor f_name in sorted(os.listdir(WEB_DIR)):\n    size = os.path.getsize(os.path.join(WEB_DIR, f_name))\n    total_size += size\n    unit = 'KB' if size < 1024*1024 else 'MB'\n    size_val = size/1024 if unit == 'KB' else size/(1024*1024)\n    print(f'  {f_name:40s} {size_val:8.1f} {unit}')\nprint(f'  {\"Total:\":40s} {total_size/1024/1024:8.1f} MB')\nprint(f'{\"=\"*60}')\nprint(f'\\n‚úÖ {len(region_map)} clinical regions mapped to FLAME topology')\nfor name, indices in sorted(region_map.items()):\n    if indices:\n        print(f'  {name:30s}: {len(indices):5d} vertices')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Preview Reconstruction"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\nfig = plt.figure(figsize=(20, 5))\n\n# 1. Source photo\nax1 = fig.add_subplot(141)\nsrc = Image.open(os.path.join(INPUT_DIR, input_files[primary_idx]))\nax1.imshow(src)\nax1.set_title('Source Photo', fontsize=12, fontweight='bold')\nax1.axis('off')\n\n# 2. 3D mesh (front view)\nax2 = fig.add_subplot(142, projection='3d')\nax2.plot_trisurf(vertices[:, 0], vertices[:, 1], vertices[:, 2],\n                 triangles=faces, color='#e8b89d', edgecolor='gray',\n                 linewidth=0.05, alpha=0.9)\nax2.set_title('3D Mesh (Front)', fontsize=12, fontweight='bold')\nax2.view_init(elev=0, azim=0)\nax2.axis('off')\nax2.set_box_aspect([1, 1.2, 1])\n\n# 3. Texture map\nax3 = fig.add_subplot(143)\ntex = Image.open(tex_path)\nax3.imshow(tex)\nax3.set_title('Albedo Texture', fontsize=12, fontweight='bold')\nax3.axis('off')\n\n# 4. 3D mesh with region coloring\nax4 = fig.add_subplot(144, projection='3d')\n# Color vertices by region\nvertex_colors = np.ones((V, 3)) * 0.85  # default gray\n\ncolor_palette = {\n    'forehead': [0.4, 0.6, 1.0],\n    'nose': [1.0, 0.7, 0.3],\n    'lips': [1.0, 0.3, 0.4],\n    'cheek_left': [0.5, 0.9, 0.5],\n    'cheek_right': [0.5, 0.9, 0.5],\n    'chin': [0.8, 0.5, 0.9],\n    'jaw_left': [0.9, 0.7, 0.5],\n    'jaw_right': [0.9, 0.7, 0.5],\n    'neck': [0.6, 0.6, 0.6],\n}\n\nfor region_name, color in color_palette.items():\n    if region_name in region_map:\n        for idx in region_map[region_name]:\n            if idx < V:\n                vertex_colors[idx] = color\n\n# Plot faces with vertex colors\nface_colors = np.mean(vertex_colors[faces], axis=1)\nax4.plot_trisurf(vertices[:, 0], vertices[:, 1], vertices[:, 2],\n                 triangles=faces, edgecolor='gray', linewidth=0.02, alpha=0.9)\nax4.set_title(f'Region Map ({len(region_map)} zones)', fontsize=12, fontweight='bold')\nax4.view_init(elev=0, azim=0)\nax4.axis('off')\nax4.set_box_aspect([1, 1.2, 1])\n\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'preview.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f'‚úÖ Preview saved to {OUTPUT_DIR}/preview.png')\nprint(f'\\nReconstruction stats:')\nprint(f'  Vertices: {vertices.shape[0]:,}')\nprint(f'  Faces: {faces.shape[0]:,}')\nprint(f'  Clinical regions: {len(region_map)}')\nprint(f'  Shape params: {len(all_params[primary_idx][\"shape\"])}')\nprint(f'  Expression params: {len(all_params[primary_idx][\"exp\"])}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 7: Download Results\n\nDownloads a ZIP containing:\n- **Traditional files**: OBJ mesh, textures, normal map, displacement map, parameters JSON\n- **Web-ready files**: Binary + JSON files for direct browser loading via `FlameMeshGenerator.loadFLAME()`\n\nUpload the `web/` folder contents to your project's `public/models/flame/web/` directory, and the textures to `public/models/`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import zipfile\n\nzip_path = '/content/facial_reconstruction_flame2023.zip'\n\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n    # Add main output files\n    for f_name in os.listdir(OUTPUT_DIR):\n        f_path = os.path.join(OUTPUT_DIR, f_name)\n        if os.path.isfile(f_path):\n            zf.write(f_path, f_name)\n\n    # Add web-ready files in web/ subdirectory\n    web_dir = os.path.join(OUTPUT_DIR, 'web')\n    if os.path.exists(web_dir):\n        for f_name in os.listdir(web_dir):\n            f_path = os.path.join(web_dir, f_name)\n            if os.path.isfile(f_path):\n                zf.write(f_path, f'web/{f_name}')\n\nzip_size = os.path.getsize(zip_path) / (1024 * 1024)\nprint(f'üì¶ Created: facial_reconstruction_flame2023.zip ({zip_size:.1f} MB)')\nprint(f'\\nContents:')\nwith zipfile.ZipFile(zip_path, 'r') as zf:\n    for info in zf.infolist():\n        size_kb = info.file_size / 1024\n        unit = 'KB' if size_kb < 1024 else 'MB'\n        size_val = size_kb if unit == 'KB' else size_kb / 1024\n        print(f'  {info.filename:40s} {size_val:8.1f} {unit}')\n\nprint(f'\\n‚¨áÔ∏è  Downloading...')\nfiles.download(zip_path)\n\nprint('\\n' + '='*60)\nprint('‚úÖ Done! Next steps:')\nprint('='*60)\nprint()\nprint('1. Unzip the downloaded file')\nprint('2. Copy web/ folder contents to your project:')\nprint('   ‚Üí facial-ai-project/public/models/flame/web/')\nprint('3. Copy textures to:')\nprint('   ‚Üí facial-ai-project/public/models/')\nprint('4. The app will auto-detect FLAME data via FlameMeshGenerator.loadFLAME()')\nprint()\nprint('üåê Your app: https://facial-ai-project.vercel.app')\nprint('üì¶ GitHub:   https://github.com/OfirVento/facial-ai-project')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}